{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7957692,"sourceType":"datasetVersion","datasetId":4680816},{"sourceId":10653461,"sourceType":"datasetVersion","datasetId":6597049},{"sourceId":11454031,"sourceType":"datasetVersion","datasetId":7176743},{"sourceId":11934914,"sourceType":"datasetVersion","datasetId":7503501},{"sourceId":11935647,"sourceType":"datasetVersion","datasetId":7503955}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nsns.set_style('whitegrid')\nfrom sklearn.metrics import confusion_matrix , classification_report\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout , Activation , BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam , Adamax\nfrom tensorflow.keras import regularizers\n\n#Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet121, ResNet50V2\n\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet169, MobileNetV2, ResNet50, EfficientNetB0\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\n\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.regularizers import l2\n\nfrom tensorflow.keras.constraints import MinMaxNorm\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.initializers import HeNormal, RandomNormal\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install lifelines","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lifelines.utils import concordance_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Modality 1","metadata":{}},{"cell_type":"code","source":"X_train_h = np.load('ham10000/x_train.npy')\ny_train_h = np.load('ham10000/y_train.npy')\nX_test_h = np.load('ham10000/x_test.npy')\ny_test_h = np.load('ham10000/y_test.npy')\n\nX_val_h = np.load('ham10000/x_val.npy')\ny_val_h = np.load('ham10000/y_val.npy')\n\n\nX_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape, X_val_h.shape, y_val_h.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(1001, 810, replace=False)\n\nX_test_h1 = X_test_h[random_indices]\ny_test_h1 = y_test_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(1002, 648, replace=False)\n\nX_val_h1 = X_val_h[random_indices]\ny_val_h1 = y_val_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape, X_val_h1.shape, y_val_h1.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Modality 2","metadata":{}},{"cell_type":"code","source":"X_train_s = np.load('SIPAKMED/features.npy')\ny_train_s = np.load('SIPAKMED/labels.npy')\n\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\nX_train_s, X_val_s, y_train_s, y_val_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\n\nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, y_val_s.shape,y_val_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by the specified angle.\n    \"\"\"\n    center = tuple(np.array(image.shape[1::-1]) / 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return rotated_image\n\ndef translate_image(image, tx, ty):\n    \"\"\"\n    Translate the image by the specified translation parameters.\n    \"\"\"\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n    return translated_image\n\ndef apply_gaussian_blur(image, kernel_size=3):\n    \"\"\"\n    Apply Gaussian Blur to the image to reduce noise and improve generalization.\n    \"\"\"\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    return blurred_image\n\n# Augmentation parameters\nrotation_angles = [20]\ntranslations = [(5, 5)]\nkernel_sizes = [3]  # Gaussian Blur kernel sizes\n\naugmented_X_train = []\naugmented_y_train = []\n\nfor image, label in zip(X_train_s, y_train_s):\n    # Augment with rotations\n    for angle in rotation_angles:\n        rotated_image = rotate_image(image, angle)\n        augmented_X_train.append(rotated_image)\n        augmented_y_train.append(label)\n\n    # Augment with translations\n    for tx, ty in translations:\n        translated_image = translate_image(image, tx, ty)\n        augmented_X_train.append(translated_image)\n        augmented_y_train.append(label)\n\n    # Augment with Gaussian Blur\n    for kernel_size in kernel_sizes:\n        blurred_image = apply_gaussian_blur(image, kernel_size)\n        augmented_X_train.append(blurred_image)\n        augmented_y_train.append(label)\n\n# Convert lists to numpy arrays\naugmented_X_train = np.array(augmented_X_train)\naugmented_y_train = np.array(augmented_y_train)\n\n# Shuffle the data\nshuffle_indices = np.random.permutation(len(augmented_X_train))\naugmented_X_train = augmented_X_train[shuffle_indices]\naugmented_y_train = augmented_y_train[shuffle_indices]\naugmented_X_train.shape, augmented_y_train.shape\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(7773, 5421, replace=False)\n\naugmented_X_train = augmented_X_train[random_indices]\naugmented_y_train = augmented_y_train[random_indices]\n\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_s = np.concatenate((X_train_s, augmented_X_train), axis=0)\ny_train_s = np.concatenate((y_train_s, augmented_y_train), axis=0)\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#del augmented_X_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Normalize from [0, 255] to [0, 1]\nX_train_s = X_train_s.astype(np.float32) / 255.0\nX_test_s = X_test_s.astype(np.float32) / 255.0\nX_val_s = X_val_s.astype(np.float32) / 255.0\n\nX_train_s.shape, X_test_s.shape, X_val_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Modality 3","metadata":{}},{"cell_type":"code","source":"X_train_multi = np.load('MIT-BIH/X_train.npy')\nX_val_multi = np.load('MIT-BIH/X_val.npy')\nX_test_multi = np.load('MIT-BIH/X_test.npy')\n\ny_train_multi = np.load('MIT-BIH/y_train.npy')\ny_val_multi = np.load('MIT-BIH/y_val.npy')\ny_test_multi = np.load('MIT-BIH/y_test.npy')\n\nX_train_multi.shape, X_val_multi.shape, X_test_multi.shape, y_train_multi.shape, y_val_multi.shape, y_test_multi.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_multi = tf.keras.utils.to_categorical(y_train_multi)\ny_val_multi = tf.keras.utils.to_categorical(y_val_multi)\n\ny_test_multi = tf.keras.utils.to_categorical(y_test_multi)\n\ny_train_multi.shape, y_val_multi.shape, y_test_multi.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Modality 4","metadata":{}},{"cell_type":"code","source":"X_train_m1 = np.load('X_train_MORT_MIMIC3_updated.npy')\ny_train_m1 = np.load('y_train_MORT_MIMIC3_updated.npy')\nX_test_m1 = np.load('X_test_MORT_MIMIC3_updated.npy')\ny_test_m1 = np.load('y_test_MORT_MIMIC3_updated.npy')\n\nX_val_m1 = np.load('X_val_MORT_MIMIC3_updated.npy')\ny_val_m1 = np.load('y_val_MORT_MIMIC3_updated.npy')\n\nX_train_m1.shape, y_train_m1.shape, X_test_m1.shape, y_test_m1.shape, X_val_m1.shape, y_val_m1.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## PCMFA\n\n\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass MACFusion(tf.keras.layers.Layer):\n    def __init__(self, units, activation='sigmoid', fractal_dim=1.8, learnable_curvature=True, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.fractal_dim = fractal_dim\n        self.learnable_curvature = learnable_curvature\n\n    def build(self, input_shape):\n        feature_dim = input_shape[0][-1]\n        self.feature_dim = feature_dim\n\n        self.global_avg_pooling = layers.GlobalAveragePooling2D()\n        self.global_max_pooling = layers.GlobalMaxPooling2D()\n        self.global_min_pooling = GlobalMinPooling2D()\n\n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n        self.global_scale1 = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale1')\n        self.global_scale2 = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale2')\n        \n        # Quantum Components\n        self.psi_real = self.add_weight(shape=(feature_dim, self.units),\n                                        initializer='glorot_uniform', name='psi_real')\n        self.psi_imag = self.add_weight(shape=(feature_dim, self.units),\n                                        initializer='glorot_uniform', name='psi_imag')\n        self.S = self.add_weight(shape=(self.units,),\n                                 initializer=tf.keras.initializers.Constant(self.fractal_dim),\n                                 constraint=MinMaxNorm(1.0, 2.0),\n                                 name='fractal_scaling')\n\n        # Hyperbolic Dual Genmetry Components\n        self.curvature_log = self.add_weight(shape=(1,), initializer='he_normal',\n                                             trainable=self.learnable_curvature,\n                                             name='curvature_log')\n        self.lorentz_alpha = self.add_weight(shape=(1,), initializer='he_normal', name='lorentz_scale')\n        self.hyper_beta = self.add_weight(shape=(feature_dim,), initializer='he_normal', name='beta_scale')\n        \n        self.hyper_gate = self.add_weight(shape=(1,), initializer='ones', name='hyper_gate')\n        self.hyper_gate1 = self.add_weight(shape=(1,), initializer='ones', name='hyper_gate1')\n\n        # Cross-modality Gates\n        self.cross_gate = self.add_weight(shape=(feature_dim,), initializer='ones', name='cross_gate')\n        self.output_gate = self.add_weight(shape=(feature_dim,), initializer='ones', name='output_gate')\n\n    def compute_curvature(self): # Compute leranable curvature read Eqs. 4 and 7 \n        return tf.clip_by_value(tf.exp(self.curvature_log), 0.1, 10.0)\n\n    # MLE of LIL block as for Eq. 7\n    def lorentz_proj(self, x1, x2, c):\n\n        space_part1 = x1\n        space_part2 = x2\n        norm_sq1 = tf.reduce_sum(x1 ** 2, axis=-1, keepdims=True)  #tf.reduce_sum(tf.square(x1), axis=-1, keepdims=True)\n        norm_sq2 = tf.reduce_sum(x2 ** 2, axis=-1, keepdims=True) #tf.reduce_sum(tf.square(x2), axis=-1, keepdims=True)\n        time1 = tf.sqrt(1 + c * norm_sq1) / c\n        time2 = tf.sqrt(1 + c * norm_sq2) / c\n        return tf.concat([time2, space_part1], axis=-1), tf.concat([time1, space_part2], axis=-1)\n        '''\n        space_part = x\n        norm_sq = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n        time_part = tf.sqrt(1 + c * norm_sq) / c\n        return tf.concat([time_part, space_part], axis=-1)\n\n        '''\n\n    '''def poincare_proj(self, x1, x2, c):\n        norm1 = tf.norm(x1, axis=-1, keepdims=True) #+ 1e-6\n        norm2 = tf.norm(x2, axis=-1, keepdims=True) #+ 1e-6\n        scale1 = tf.tanh(tf.sqrt(c) * norm1) / norm1\n        scale2 = tf.tanh(tf.sqrt(c) * norm2) / norm2\n        return x1 * scale1, x2 * scale2\n        '''\n    # MPE of PIL block as for Eq. 4\n    def poincare_proj(self, x1, x2, c):\n        norm1 = tf.norm(x1, axis=-1, keepdims=True) #+ 1e-6\n        norm2 = tf.norm(x2, axis=-1, keepdims=True) #+ 1e-6\n        scale1 = tf.tanh(tf.sqrt(c) * norm1) * (x1 / (norm1 + 1e-6)) #tf.tanh(tf.sqrt(c) * norm1) / norm1\n        scale2 = tf.tanh(tf.sqrt(c) * norm2) * (x2 / (norm2 + 1e-6))  #tf.tanh(tf.sqrt(c) * norm2) / norm2\n        return x1 * scale1, x2 * scale2\n\n    def call(self, inputs):\n        input1, input2 = inputs\n\n        \n        input_shape = tf.shape(input1)\n        \n        #flattened_inputs1 = tf.reshape(input1, [-1, input_shape[-1]])  # Flatten along the last axis\n\n        # Feature flattening\n        x1_flat = tf.reduce_mean(input1, axis=[1, 2]) if len(input1.shape) == 4 else input1\n        x2_flat = tf.reduce_mean(input2, axis=[1, 2]) if len(input2.shape) == 4 else input2\n\n        ## Use DCT for capturing frequency information\n        x1_flat = tf.signal.dct(x1_flat, type=2, norm='ortho')\n        x2_flat = tf.signal.dct(x2_flat, type=2, norm='ortho')\n\n        \n\n        #. Multimodal Hyperbolic Dual-Geometry Attention (MHDGA)\n        c = self.compute_curvature()\n        adjusted_S = self.S * tf.sigmoid(c)\n        adjusted_c = c * tf.reduce_mean(tf.sigmoid(self.S))\n\n        \n        # MQIA for estimating quantum states for each modality input\n        psi1 = tf.complex(tf.matmul(x1_flat, self.psi_real), tf.matmul(x1_flat, self.psi_imag))\n        psi2 = tf.complex(tf.matmul(x2_flat, self.psi_real), tf.matmul(x2_flat, self.psi_imag))\n\n        \n        # Mutual Guidance for MQIA through MLE of LIL block for each modality input as self.lorentz_proj() used in LIL block as for Eq. 9\n        h_psi1, h_psi2 = self.lorentz_proj(tf.math.real(psi1), tf.math.real(psi2), adjusted_c)\n        prob1 = tf.math.real(psi1 * tf.math.conj(psi1)) #+ 1e-8\n        prob2 = tf.math.real(psi2 * tf.math.conj(psi2)) #+ 1e-8\n        lorentz_factor1 = tf.norm(h_psi1[..., 1:], axis=-1, keepdims=True)\n        lorentz_factor2 = tf.norm(h_psi2[..., 1:], axis=-1, keepdims=True)\n\n        ## Attention maps learned from MQIA for each modality input as for Eq. 9\n        \n        scaled_prob1 = prob1 * tf.pow(lorentz_factor1, adjusted_S - 2)  \n        scaled_prob2 = prob2 * tf.pow(lorentz_factor2, adjusted_S - 2)\n        \n        fractal_att1 = tf.nn.softmax(scaled_prob1, axis=-1)\n        fractal_att2 = tf.nn.softmax(scaled_prob2, axis=-1)\n\n        \n        # Multimodal Hyperbolic Dual-Geometry Attention (MHDGA)\n        h_lorentz1, h_lorentz2 = self.lorentz_proj(x1_flat, x2_flat, adjusted_c) # for LIL block for hyperbloc space computation as for Eq. 7\n        h_poincare1, h_poincare2 = self.poincare_proj(x1_flat, x2_flat, adjusted_c) # for PIL block for hyperbloc space computation as for Eq. 5\n\n        ## Ref. Eq. 6 to estimate times and frequency components for LIL:\n        t1, x1 = h_lorentz1[..., :1], h_lorentz1[..., 1:]\n        t2, x2 = h_lorentz2[..., :1], h_lorentz2[..., 1:]\n\n\n        ## Bais estimation from MQIA for LIL block to generate attention maps see Eq. 8:\n        quantum_bias1 = tf.matmul(fractal_att1, tf.transpose(self.psi_real))\n        quantum_bias2 = tf.matmul(fractal_att2, tf.transpose(self.psi_real))\n\n\n        \n\n        # Lorentz attention weights is computed via Minkowski inner product (see Eq. 8):\n\n        lorentz_att1 = self.lorentz_alpha * (-t1**2 + tf.reduce_sum((x1 + quantum_bias1)**2 * self.hyper_beta, axis=-1, keepdims=True))\n        lorentz_att2 = self.lorentz_alpha * (-t2**2 + tf.reduce_sum((x2 + quantum_bias2)**2 * self.hyper_beta, axis=-1, keepdims=True))\n\n        # Poincare attention weights is computed (see Eq. 5):\n        poincare_att1 = tf.reduce_sum(h_poincare1 * x1_flat, axis=-1, keepdims=True)\n        poincare_att2 = tf.reduce_sum(h_poincare2 * x2_flat, axis=-1, keepdims=True)\n\n        #gate_factor = tf.tanh(self.hyper_gate)\n        gate_factor = self.hyper_gate\n        gate_factor1 = self.hyper_gate1\n\n        # dual-geometry attention maps (see Eq. 3):\n        hyper_att1 = tf.nn.sigmoid(gate_factor * lorentz_att1 + gate_factor1 * poincare_att1)\n        hyper_att2 = tf.nn.sigmoid(gate_factor * lorentz_att2 + gate_factor1 * poincare_att2)\n\n        # Reshape attention maps\n        def expand(x, target_shape):\n            if len(target_shape) == 4:\n                x = tf.expand_dims(tf.expand_dims(x, 1), 1)\n                return tf.tile(x, [1, target_shape[1], target_shape[2], 1])\n            return x\n\n        fractal_att1 = expand(fractal_att1, input1.shape)\n        fractal_att2 = expand(fractal_att2, input1.shape)\n        \n        hyper_att1 = expand(hyper_att1, input1.shape)\n        hyper_att2 = expand(hyper_att2, input2.shape)\n        \n        \n        cross_gate = tf.reshape(self.cross_gate, (1, 1, 1, -1))\n        output_gate1 = tf.reshape(self.output_gate, (1, 1, 1, -1))\n        output_gate2 = tf.reshape(self.output_gate, (1, 1, 1, -1))\n        output_gate3 = tf.reshape(self.output_gate, (1, 1, 1, -1))\n        output_gate4 = tf.reshape(self.output_gate, (1, 1, 1, -1))\n\n        #hyper_att = hyper_att1 + hyper_att2\n        \n        # Cross Fusion (see Eqs. 1, 2, 10):\n        fractal_out1 = input2 * cross_gate * fractal_att1 #* attention1 #* hyper_att1\n        hyper_out1 = input2 * cross_gate * hyper_att1 #* attention1\n        \n        fractal_out2 = input1 * cross_gate * fractal_att2 #* attention2 #* hyper_att1\n        hyper_out2 = input1 * cross_gate * hyper_att2 #* attention2\n        \n        #  Multimodal Attention Fusion Gating Block:\n        return ((output_gate1 * fractal_out2) + (output_gate2 * hyper_out2)), ((output_gate3 * fractal_out1) + (output_gate4 * hyper_out1)) #+ (output_gate4 * attn1) + (output_gate4 * attn2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## EXtending MHCF for devsiing SIR module\ndef multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, groups=groups, padding=\"same\")(x)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n    conv_dilated = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    # Downsampling x1\n    x1 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x1)\n\n    # **Fix: Ensure x has the same number of channels as x1**\n    x = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x)\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # Residual Connection\n    x = layers.Add()([x, x1])\n    x = layers.Activation('gelu')(x)  # GELU activation for better convergence\n    \n    return x\n\n\n\n\n# MHCF for EMRC block\ndef multi_kernel_groupwise_conv3(x, filters, groups=8, strides=2, use_se=True):\n    # GPC\n    conv1x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv1x1 = layers.BatchNormalization()(conv1x1)\n\n    # DDC\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=1,strides=strides,  padding=\"same\", use_bias=False)(x)\n    \n    #conv3x3 = layers.GaussianBlur(kernel_size=3, sigma=1.0)(conv3x3)\n    #if strides==2:\n     #   conv3x3 = layers.AveragePooling2D(2, 2, padding=\"same\")(conv3x3)\n\n    conv3x3 = layers.BatchNormalization()(conv3x3)\n\n    # DWC\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv5x5 = layers.BatchNormalization()(conv5x5)\n\n    # Concatenation and 1x1 Fusion\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    \n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\", use_bias=False)(x1)\n    \n    x1 = layers.BatchNormalization()(x1)\n\n    # Residual Connection\n    x = layers.Conv2D(filters=filters, kernel_size=1, strides=strides, groups= 8, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)  # GELU activation for better convergence\n\n    return x\n\n\n# Single branch of EMRC for each modality input, such as x_i\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First MHCF\n    x = multi_kernel_groupwise_conv3(x, filters=filters, groups=8, strides=strides)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second MHCF\n    x = multi_kernel_groupwise_conv3(x, filters=filters, groups=8, strides=(1, 1))  # Strides=1 to avoid mismatch\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)  # Pointwise conv for channel mixing\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed (Ensure Matching Shape)\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, groups=8, padding=\"same\")(shortcut)  # Downsampling shortcut\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    #x, shortcut = MACFusion(units=filters)([x, shortcut])\n    # Residual Connection (Ensure Same Shape)\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def EMRC(x1, x2, filters, strides):\n    x1 = RGSA(x1, filters=filters, strides=strides, use_projection=True)\n    x2 = RGSA(x2, filters=filters, strides=strides, use_projection=True)\n    return x1, x2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def info_fusion(x_1, x_2, filter):\n    x11 = multi_kernel_groupwise_conv1(x_1, filters=filter, groups=8, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv1(x_2, filters=filter, groups=8, strides=(2,2))\n\n    return x11, x21\n\ndef info_fusion1(x_1, x_2, filter):\n    x11 = multi_kernel_groupwise_conv1(x_1, filters=filter, groups=8, strides=(1,1))\n    x21 = multi_kernel_groupwise_conv1(x_2, filters=filter, groups=8, strides=(1,1))\n\n    return x11, x21","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def SIR(x1, x2, f):\n    if f == 64:\n        x111, x211 = info_fusion(x1, x2, 2 * f)\n        x111, x211 = info_fusion(x111, x211, 4 * f)\n        x111, x211 = info_fusion(x111, x211, 8 * f)\n        return x111, x211\n\n    elif f == 128:\n        x111, x211 = info_fusion(x1, x2, 2 * f)\n        x111, x211 = info_fusion(x111, x211, 4 * f)\n        return x111, x211\n\n    elif f == 256:\n        x111, x211 = info_fusion(x1, x2, 2 * f)\n        return x111, x211\n\n    elif f == 512:\n        x111, x211 = info_fusion1(x1, x2, f)\n        return x111, x211\n\n    else:\n        raise ValueError(f\"Unsupported fusion factor: {f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def PCMFA(x1, x2, f):\n    \n    x1, x2 = MACFusion(units=f)([x1, x2]) # First PCMFA\n    x1, x2 = MACFusion(units=f)([x1, x2]) # Second PCMFA\n\n    return x1, x2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def EHF1(x1, x2, f, s):\n    x1, x2 = EMRC(x1, x2, f, s) # Call EMRC module\n    x1, x2 = PCMFA(x1, x2, f) # # Call PCMFA module\n    return x1, x2    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.initializers import Ones, Constant\n\nclass LF(layers.Layer):\n    def build(self, input_shape):\n        # input_shape: list of two shapes, each (B, H, W, C)\n        # we use a single scalar per tensor (broadcastable to all channels)\n        \n\n        self.global_scale1 = self.add_weight(\n            name=\"global_scale1\",\n            shape=(1,1,1,1),\n            initializer=Ones(),      # starts at 1.0\n            trainable=True\n        )\n        # or, if you want a tiny deviation around 1:\n        self.global_scale2 = self.add_weight(\n            name=\"global_scale2\",\n            shape=(1,1,1,1),\n            initializer=Constant(value=1.0),  \n            # this gives values ~ N(1,0.01)\n            trainable=True\n        )\n\n    def call(self, inputs):\n        x1, x2 = inputs\n        # scale each branch by its learnable scalar\n        x1_scaled = x1 * self.global_scale1\n        x2_scaled = x2 * self.global_scale2\n        # concatenate along channel axis\n        return layers.Concatenate(axis=-1)([x1_scaled, x2_scaled])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def EHF2(x1, x2, f, s):\n    x1, x2 = EMRC(x1, x2, f, s) # Call EMRC module\n    \n    x1 = layers.BatchNormalization()(x1)\n    x2 = layers.BatchNormalization()(x2)\n    \n    x1, x2 = PCMFA(x1, x2, f) # Call PCMFA module\n    \n    x111, x211 = SIR(x1, x2, f) ## Call SIR module\n\n    #x1 = layers.Concatenate(axis = -1)([x1, x2])\n    \n    \n    x1 = LF()([x1, x2])  # Use Learnable Late Fusion (LF) for capturing multimodal information\n    \n    x1 = layers.Conv2D(f, kernel_size=(1, 1), groups=8, \n                       padding=\"same\")(x1) \n    \n    return x1, x111, x211    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def EHF(x1, x2, filters, strides1, strides2):\n    if filters == 64:\n        x1, x2 = EHF1(x1, x2, filters, strides2)\n        x1, x2 = EHF1(x1, x2, filters, strides2)\n        return x1, x2\n\n    elif filters == 128:\n        x1, x2 = EHF1(x1, x2, filters, strides1)\n        x1, x111, x211 = EHF2(x1, x2, filters, strides2)\n        return x1, x111, x211\n\n    elif filters == 256:\n        x1, x2 = EHF1(x1, x2, filters, strides1)\n        x1, x111, x211 = EHF2(x1, x2, filters, strides2)\n        return x1, x111, x211\n\n    elif filters == 512:\n        x1, x2 = EHF1(x1, x2, filters, strides1)\n        x1, x111, x211 = EHF2(x1, x2, filters, strides2)\n        return x1, x111, x211\n\n    else:\n        return x1, x2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def residual_GLC_branch1(inputs1, inputs2, inputs3, inputs4):\n\n    # Modality input 1\n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n\n    # Modality input 2\n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n\n    \n    x1, x2 = EHF(x1, x2, filters=64, strides1=1, strides2=1)\n\n    x1, x111, x211 = EHF(x1, x2, filters = 128, strides1=1, strides2=2)\n\n\n    # Modality input 3\n    x3 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs3)\n    x3 = BatchNormalization()(x3)\n    x3 = tf.keras.layers.Activation('relu')(x3)\n    x3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x3)\n\n    x2 = multi_kernel_groupwise_conv3(x3, filters=256, groups=8, strides=2)\n    \n    x1, x1111, x2111 = EHF(x1, x2, filters = 256, strides1=1, strides2=2)\n\n    \n    # Modality input 4\n    x4 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs4)\n    x4 = BatchNormalization()(x4)\n    x4 = tf.keras.layers.Activation('relu')(x4)\n    x4 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x4)\n    \n    \n    x2 = multi_kernel_groupwise_conv3(x4, filters=512, groups=8, strides=2)\n    x2 = multi_kernel_groupwise_conv3(x2, filters=512, groups=8, strides=2)\n    \n    #x2 = RGSA(x2, filters=512, strides=(2, 2), use_projection=True)\n\n    x1, x11111, x21111 = EHF(x1, x2, filters = 512, strides1=1, strides2=2)\n    \n    \n    return x1, x111, x211, x1111, x2111, x11111, x21111","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''class CIndexCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        # Predict risk scores on the full validation set\n        pred = self.model.predict(X_val_multi, verbose=0).reshape(-1)\n        #risk_scores = preds[2].reshape(-1)\n        durs = y_val_tab[:, 0]\n        evts = y_val_tab[:, 1].astype(bool)\n        ci = concordance_index(durs, -pred, evts)\n        logs = logs or {}\n        logs['val_c_index'] = ci\n        print(f\" — val_c_index: {ci:.4f}\")\n'''\n\n'''class CIndexCallback(tf.keras.callbacks.Callback):\n    def __init__(self, val_inputs, val_cox_y):\n        \"\"\"\n        val_inputs: list of your four validation inputs,\n                    e.g. [X_val_s, X_val_h1, X_val_multi, X_val_m1]\n        val_cox_y:  the (n,2) array of [durations, events] for the Cox head\n        \"\"\"\n        super().__init__()\n        self.val_inputs = val_inputs\n        self.val_cox_y   = val_cox_y\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        # 1) predict returns a list of outputs, one per head\n        preds = self.model.predict(self.val_inputs, batch_size = 4, verbose=0)\n        # 2) pick out the Cox‐head predictions (assuming it's the 3rd head: index 2)\n        risk_scores = preds[2].reshape(-1)\n\n        # 3) pull durations/events\n        durations = self.val_cox_y[:, 0]\n        events    = self.val_cox_y[:, 1].astype(bool)\n\n        # 4) compute C‐index\n        ci = concordance_index(durations, -risk_scores, events)\n        logs['val_c_index'] = ci\n\n        print(f\" — val_c_index: {ci:.4f}\")\n\n\n#cidx_cb = CIndexCallback()\n\nval_inputs = [X_val_s, X_val_h1, X_val_multi, X_val_m1]\n# the Cox‐head targets for val:\nval_cox_y = y_val_multi  # shape (n_val, 2)\n\ncidx_cb = CIndexCallback(val_inputs, val_cox_y)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''def neg_log_partial_likelihood(y_true, y_pred):\n    eps = 1e-8\n\n    # unpack\n    durations = y_true[:, 0]\n    events    = y_true[:, 1]\n\n    # sort by descending time\n    order = tf.argsort(durations, direction=\"DESCENDING\")\n    p = tf.gather(tf.reshape(y_pred, [-1]), order)\n\n    # clip the raw predictions to a safe window before exp\n    p = tf.clip_by_value(p, -10.0, 10.0)\n\n    # hazard ratio\n    hr = tf.exp(p)\n    hr = tf.clip_by_value(hr, eps, 1e6)\n\n    # cumulative hazard\n    cum_h = tf.cumsum(hr) + eps\n    log_cum = tf.math.log(cum_h)\n\n    diff = p - log_cum\n    loss = -tf.reduce_sum(diff * tf.gather(events, order))\n\n    return loss / (tf.reduce_sum(events) + eps)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ndef build_model():\n\n    input_shape=(128, 128, 3)\n    inputs1 = Input(shape=input_shape)\n    inputs2 = Input(shape=input_shape)\n    inputs3 = Input(shape=input_shape)\n    inputs4 = Input(shape=input_shape)\n    \n    import tensorflow.keras.layers as L\n    \n    #input_data = Input(shape=input_shape, name='input_data')\n    # Initial convolutional layer\n    \n    #x1, x2 = residual_GLC_branch1(inputs1, inputs2)\n    \n    x1, x111, x211, x1111, x2111, x11111, x21111 = residual_GLC_branch1(inputs1, inputs2, inputs3, inputs4)\n    #print('x:',x.shape)\n    \n    con1 = tf.keras.layers.Concatenate(axis=-1)([x111, x1111, x11111])\n    con2 = tf.keras.layers.Concatenate(axis=-1)([x211, x2111, x21111])\n\n    con = tf.keras.layers.Concatenate(axis=-1)([x1, con1, con2])\n\n    #con = tf.keras.layers.Concatenate(axis=-1)([x1, x111, x1111, x11111, x211, x2111, x21111])\n    \n    con = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n    \n    x = GlobalAveragePooling2D()(con)\n    \n    \n    outputs1 = Dense(5, activation='softmax')(x) # Output layer for modality input 1\n    outputs2 = Dense(7, activation='softmax')(x) # Output layer for modality input 2\n    outputs3 = Dense(2, activation=\"sigmoid\")(x) # Output layer for modality input 3\n    outputs4 = Dense(2, activation='sigmoid')(x) # Output layer for modality input 4\n    \n    # Create the model\n    model = Model([inputs1, inputs2, inputs3, inputs4], [outputs1, outputs2, outputs3, outputs4])\n    #return model\n    #print(model.summary())\n\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n    initial_gamma1 = 0.25\n    initial_gamma2 = 0.25\n    initial_gamma3 = 0.25\n    \n    \n    \n    from tensorflow.keras.optimizers import Adam\n\n    #opt = Adam(learning_rate=1e-4, clipnorm=1.0)\n    \n    #opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n    \n    # Compile the model with the custom optimizer\n    model.compile(optimizer='adam', loss=['categorical_crossentropy', 'categorical_crossentropy', #neg_log_partial_likelihood, \n                                          'binary_crossentropy', 'binary_crossentropy'],\n                  loss_weights=[initial_gamma1, initial_gamma2, initial_gamma3, (1 -  (initial_gamma1 + initial_gamma2 + initial_gamma3))],\n                  metrics=['accuracy','accuracy','accuracy','accuracy']\n                  #metrics={'h':    ['accuracy', tf.keras.metrics.AUC(name='auc')], 's':    ['accuracy', tf.keras.metrics.AUC(name='auc')],\n            #'multi':['accuracy', tf.keras.metrics.AUC(name='auc')], 'm1':   ['accuracy', tf.keras.metrics.AUC(name='auc')]}\n                 )\n       \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = build_model()\n\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.000001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [#cidx_cb, \n             checkpoint_callback, early_stopping, reduce_lr]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h, X_train_multi, X_train_m1], [y_train_s, y_train_h, y_train_multi, y_train_m1],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1, X_val_multi, X_val_m1], [y_val_s, y_val_h1, y_val_multi, y_val_m1]), verbose=1, \n                    shuffle=True, #batch_size=40,\n                    callbacks=callbacks) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1, X_test_multi, X_test_m1], [y_test_s, y_test_h1, y_test_multi, y_test_m1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport random\n\n# 1) Generate or define your five seeds\nimport random\n\n# 1) Generate five random 32-bit integer seeds\n#    (you can set a seed here if you want reproducible seed-generation)\nseeds = np.random.randint(0, 2**31 - 1, size=5).tolist()\nprint(\"Using seeds:\", seeds)\n\n\n# 2) Prepare empty lists for each modality\nmodalities = ['Dermoscopy', 'Cytology', 'ECG', 'EHR']\nstats = {\n    mod: {\n        'accs': [],\n        'aucs': []\n    } for mod in modalities\n}\n\nfor seed in seeds:\n    # ── reproducibility ───────────────────────────────────────────\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # ── build & compile ───────────────────────────────────────────\n    model = build_model()  # your model builder\n    model.compile(\n        optimizer='adam',\n        loss={\n            'h':    'binary_crossentropy',\n            's':    'binary_crossentropy',\n            'multi':'binary_crossentropy',\n            'm1':   'binary_crossentropy'\n        },\n        metrics={\n            'h':    ['accuracy', tf.keras.metrics.AUC(name='auc')],\n            's':    ['accuracy', tf.keras.metrics.AUC(name='auc')],\n            'multi':['accuracy', tf.keras.metrics.AUC(name='auc')],\n            'm1':   ['accuracy', tf.keras.metrics.AUC(name='auc')]\n        }\n    )\n\n    # ── fit ───────────────────────────────────────────────────────\n    model.fit(\n        [X_train_h, X_train_s, X_train_multi, X_train_m1],\n        [y_train_h, y_train_s, y_train_multi, y_train_m1],\n        epochs=200,\n        validation_data=(\n            [X_val_h1, X_val_s, X_val_multi, X_val_m1],\n            [y_val_h1, y_val_s, y_val_multi, y_val_m1]\n        ),\n        verbose=0,\n        shuffle=True,\n        callbacks=callbacks\n    )\n\n    # ── evaluate ─────────────────────────────────────────────────\n    names, results = model.metrics_names, model.evaluate(\n        [X_test_h1, X_test_s, X_test_multi, X_test_m1],\n        [y_test_h1, y_test_s, y_test_multi, y_test_m1],\n        verbose=0\n    )\n    res = dict(zip(names, results))\n\n    # ── collect per‐modality ──────────────────────────────────────\n    for mod in modalities:\n        stats[mod]['accs'].append(res[f\"{mod}_accuracy\"])\n        stats[mod]['aucs'].append(res[f\"{mod}_auc\"])\n\n# 3) Compute & print modality-wise mean ± std\nfor mod in modalities:\n    accs = np.array(stats[mod]['accs'])\n    aucs = np.array(stats[mod]['aucs'])\n    print(f\"Modality {mod.upper()}:\")\n    print(f\"  ACC: {accs.mean():.4f} ± {accs.std():.4f}\")\n    print(f\"  AUC: {aucs.mean():.4f} ± {aucs.std():.4f}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}